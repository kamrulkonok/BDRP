{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61546e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60c5be03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1\n",
      "CUDA version: 11.8\n",
      "Is CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89b6dda3-50b2-4cd3-a607-936e835c45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch.optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import faiss\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "import math\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import multiprocessing\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a03cf3b3-ae1a-4528-8dc2-b410dbc2ac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 310554 images.\n"
     ]
    }
   ],
   "source": [
    "# Define custom dataset class for Xray images\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Initial transform for grayscale images\n",
    "initial_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load dataset for grayscale images\n",
    "dataset_path = \"/gpfs/workdir/islamm/rotated_datasets_without_NoFindings\" \n",
    "dataset = ChestXrayDataset(root_dir=dataset_path, transform=initial_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=False, num_workers=8, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "print(\"Dataset loaded with {} images.\".format(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9b3accd-b864-4498-9ff8-785890f4c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AlexNet model definition\n",
    "__all__ = [ 'AlexNet', 'alexnet']\n",
    " \n",
    "# (number of filters, kernel size, stride, pad)\n",
    "CFG = {\n",
    "    '2012': [(96, 11, 4, 2), 'M', (256, 5, 1, 2), 'M', (384, 3, 1, 1), (384, 3, 1, 1), (256, 3, 1, 1), 'M']\n",
    "}\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, features, num_classes, sobel):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(nn.Dropout(0.5),\n",
    "                            nn.Linear(256 * 6 * 6, 4096),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            nn.Dropout(0.5),\n",
    "                            nn.Linear(4096, 4096),\n",
    "                            nn.ReLU(inplace=True))\n",
    "\n",
    "        self.top_layer = nn.Linear(4096, num_classes)\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if sobel:\n",
    "            sobel_filter = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1) \n",
    "            sobel_filter.weight.data[0, 0].copy_(\n",
    "                torch.FloatTensor([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "            )\n",
    "            sobel_filter.weight.data[1, 0].copy_(\n",
    "                torch.FloatTensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "            )\n",
    "            sobel_filter.bias.data.zero_()\n",
    "            self.sobel = sobel_filter\n",
    "            for p in self.sobel.parameters():\n",
    "                p.requires_grad = False\n",
    "        else:\n",
    "            self.sobel = None\n",
    "\n",
    "    def forward(self, x, return_features=True):\n",
    "        if self.sobel:\n",
    "            x = self.sobel(x)\n",
    "        x = self.features(x)\n",
    "        if return_features:\n",
    "            return x.view(x.size(0), -1) \n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        if self.top_layer:\n",
    "            x = self.top_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "def make_layers_features(cfg, input_dim, bn):\n",
    "    layers = []\n",
    "    in_channels = input_dim\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=3, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v[0], kernel_size=v[1], stride=v[2], padding=v[3])\n",
    "            if bn:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v[0]), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v[0]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def alexnet(sobel=True, bn=True, out=1000):\n",
    "    dim = 2 + int(not sobel)\n",
    "    model = AlexNet(make_layers_features(CFG['2012'], dim, bn=bn), out, sobel)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ee0d818-183b-415b-8729-6fa9d73dfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def compute_features(dataloader, model, N, device):\n",
    "    \"\"\"\n",
    "    Compute features for the entire dataset and store in a pre-allocated numpy array.\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for the dataset.\n",
    "        model (nn.Module): CNN model used to extract features.\n",
    "        N (int): Total number of samples in the dataset.\n",
    "        device (torch.device): GPU or CPU device.\n",
    "    Returns:\n",
    "        np.ndarray: Numpy array of extracted features.\n",
    "    \"\"\"\n",
    "    print(\"Computing features...\")\n",
    "    model.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    # Pre-allocate numpy array for features\n",
    "    for i, input_tensor in enumerate(tqdm(dataloader, desc=\"Feature Extraction\")):\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        with torch.no_grad():\n",
    "            aux = model(input_tensor).cpu().numpy()\n",
    "\n",
    "        # Initialize feature matrix on the first batch\n",
    "        if i == 0:\n",
    "            features = np.zeros((N, aux.shape[1]), dtype='float32')\n",
    "\n",
    "        # Save extracted features\n",
    "        if i < len(dataloader) - 1:\n",
    "            features[i * dataloader.batch_size: (i + 1) * dataloader.batch_size] = aux\n",
    "        else:\n",
    "            features[i * dataloader.batch_size:] = aux\n",
    "\n",
    "        # Measure batch time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    print(f\"Feature extraction completed. Total time: {batch_time.sum:.2f} seconds\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3690fc0-d6c2-4ffd-834c-c5e4351f9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features, pca_dim):\n",
    "    \"\"\"\n",
    "    Preprocess features using PCA, whitening, and L2 normalization.\n",
    "    Args:\n",
    "        features (torch.Tensor): Raw features extracted from the model.\n",
    "        pca_dim (int): Target dimensionality after PCA.\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed features ready for clustering.\n",
    "    \"\"\"\n",
    "    features = features.astype('float32')\n",
    "\n",
    "    # Apply PCA and whitening with Faiss\n",
    "    ndim = features.shape[1]\n",
    "    pca = faiss.PCAMatrix(ndim, pca_dim, eigen_power=-0.5)\n",
    "    pca.train(features)\n",
    "    features = pca.apply_py(features)\n",
    "\n",
    "    # L2 normalization\n",
    "    features /= np.linalg.norm(features, axis=1, keepdims=True)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "216cc845-8ef3-4745-a977-57fd4bfb358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(features, num_clusters, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform k-means clustering on preprocessed features using FAISS GPU.\n",
    "    Args:\n",
    "        features (np.ndarray): Preprocessed feature vectors.\n",
    "        num_clusters (int): Number of clusters.\n",
    "        verbose (bool): If True, print k-means loss evolution.\n",
    "    Returns:\n",
    "        list: Cluster assignments for each feature vector.\n",
    "        float: Final k-means loss.\n",
    "    \"\"\"\n",
    "    n_data, d = features.shape\n",
    "\n",
    "    # Configure k-means clustering\n",
    "    clus = faiss.Clustering(d, num_clusters)\n",
    "    clus.seed = np.random.randint(1234)\n",
    "    clus.niter = 20\n",
    "    clus.max_points_per_centroid = 10000000\n",
    "\n",
    "    # Set GPU resources and configuration\n",
    "    res = faiss.StandardGpuResources()\n",
    "    flat_config = faiss.GpuIndexFlatConfig()\n",
    "    flat_config.useFloat16 = True  # Enable float16 for speed\n",
    "    flat_config.device = 0  # Set to GPU 0\n",
    "\n",
    "    # Create the FAISS index\n",
    "    index = faiss.GpuIndexFlatL2(res, d, flat_config)\n",
    "\n",
    "    # Perform clustering\n",
    "    clus.train(features, index)\n",
    "    _, I = index.search(features, 1)  # Get cluster assignments\n",
    "    losses = faiss.vector_to_array(clus.obj)  # K-means loss evolution\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"k-means loss evolution: {losses}\")\n",
    "\n",
    "    return I.flatten().tolist(), losses[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a0edd65-98f2-4532-ae0a-c8d2af6ab556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReassignedDataset class\n",
    "class ReassignedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset with images assigned pseudo-labels from clustering.\n",
    "    Args:\n",
    "        image_paths (list): List of image file paths.\n",
    "        pseudo_labels (list): Cluster assignments (pseudo-labels).\n",
    "        transform (callable, optional): Transformation pipeline for images.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, pseudo_labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.pseudo_labels = pseudo_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        pseudo_label = self.pseudo_labels[idx]\n",
    "        img = Image.open(img_path).convert(\"L\") \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, pseudo_label\n",
    "\n",
    "# Modify the cluster_assign function to use the correct arguments\n",
    "def cluster_assign(images_lists, dataset, transform=None):\n",
    "    \"\"\"Creates a dataset from clustering, with clusters as labels.\n",
    "    Args:\n",
    "        images_lists (list of list): For each cluster, the list of image indexes\n",
    "                                     belonging to this cluster.\n",
    "        dataset (Dataset): Initial dataset (ChestXrayDataset or similar).\n",
    "        transform (callable, optional): Image transformation pipeline.\n",
    "    Returns:\n",
    "        ReassignedDataset: A dataset with clusters as labels (pseudo-labels).\n",
    "    \"\"\"\n",
    "    assert images_lists is not None\n",
    "\n",
    "    # Initialize lists for storing image indices and their corresponding pseudo-labels\n",
    "    pseudolabels = []\n",
    "    image_indexes = []\n",
    "\n",
    "    # Iterate through clusters and assign pseudo-labels\n",
    "    for cluster, images in enumerate(images_lists):\n",
    "        image_indexes.extend(images)\n",
    "        pseudolabels.extend([cluster] * len(images))\n",
    "\n",
    "    # If no transform is provided, use the dataset's transform (this can be modified)\n",
    "    if transform is None:\n",
    "        transform = dataset.transform  # Use the same transform as defined in ChestXrayDataset\n",
    "\n",
    "    # Create and return the ReassignedDataset with pseudo-labels\n",
    "    return ReassignedDataset(dataset.image_paths, pseudolabels, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04501775-60ca-4fc8-9544-49f72fded4aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Epoch 1/10 ###\n",
      "Extracting features...\n",
      "Computing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|█████████████████████████████████████████████████████| 607/607 [1:45:25<00:00, 10.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed. Total time: 6325.17 seconds\n",
      "Preprocessing features...\n",
      "Clustering into 1000 clusters...\n"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "NUM_EPOCHS = 10  # Total number of epochs\n",
    "NUM_CLUSTERS = 1000  # Number of clusters for k-means\n",
    "PCA_DIM = 256  # Dimensionality for PCA\n",
    "BATCH_SIZE = 512  # Batch size for pseudo-labeled training\n",
    "LEARNING_RATE = 0.05  # Learning rate for SGD\n",
    "WEIGHT_DECAY = 1e-5  # Weight decay for regularization\n",
    "\n",
    "# Instantiate the model\n",
    "model = alexnet(sobel=True, bn=True, out=1000) \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Initialize previous cluster assignments for NMI calculation\n",
    "prev_cluster_assignments = None\n",
    "# Start training loop with PyTorch profiler\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "             schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "             on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),\n",
    "             record_shapes=True,\n",
    "             with_stack=True) as prof:\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n### Epoch {epoch + 1}/{NUM_EPOCHS} ###\")\n",
    "\n",
    "        # Step 1: Extract features from the dataset\n",
    "        with record_function(\"Extract Features\"):\n",
    "            print(\"Extracting features...\")\n",
    "            features = compute_features(dataloader, model, len(dataset), device)  # Get features from hidden layers\n",
    "        \n",
    "        # Step 2: Preprocess features (PCA, whitening, normalization)\n",
    "        with record_function(\"Preprocess Features\"):\n",
    "            print(\"Preprocessing features...\")\n",
    "            preprocessed_features = preprocess_features(features, pca_dim=PCA_DIM)\n",
    "        \n",
    "        # Step 3: Perform k-means clustering\n",
    "        with record_function(\"K-means Clustering\"):\n",
    "            print(f\"Clustering into {NUM_CLUSTERS} clusters...\")\n",
    "            cluster_assignments = run_kmeans(preprocessed_features, NUM_CLUSTERS)\n",
    "        \n",
    "        # Step 4: Convert cluster assignments to images_lists\n",
    "        images_lists = [[] for _ in range(NUM_CLUSTERS)]\n",
    "        for idx, cluster_id in enumerate(cluster_assignments):\n",
    "            images_lists[cluster_id].append(idx)\n",
    "        \n",
    "        # Step 5: Assign pseudo-labels to dataset\n",
    "        with record_function(\"Assign Pseudo-labels\"):\n",
    "            print(\"Assigning pseudo-labels...\")\n",
    "            pseudo_dataset = cluster_assign(images_lists, dataset, transform=initial_transform)\n",
    "            pseudo_dataloader = DataLoader(pseudo_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "        \n",
    "        # Step 6: Train CNN with pseudo-labeled dataset\n",
    "        with record_function(\"Train CNN\"):\n",
    "            print(\"Training CNN with pseudo-labels...\")\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for imgs, pseudo_labels in tqdm(pseudo_dataloader, desc=\"Training\"):\n",
    "                imgs, pseudo_labels = imgs.to(device), pseudo_labels.to(device)\n",
    "\n",
    "                # Forward pass (using features from the conv5 layer)\n",
    "                features = model(imgs, return_features=True)  # Get hidden features (shape: batch_size, 256, 6, 6)\n",
    "                \n",
    "                # Flatten the conv5 features\n",
    "                features = features.view(features.size(0), -1)  # Shape: (batch_size, 256*6*6)\n",
    "                \n",
    "                # Compute the loss using the flattened features\n",
    "                loss = criterion(features, pseudo_labels)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_loss = running_loss / len(pseudo_dataloader)\n",
    "            print(f\"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Step 7: Evaluate clustering performance (optional)\n",
    "        if prev_cluster_assignments is not None:\n",
    "            with record_function(\"Evaluate Clustering\"):\n",
    "                # Calculate NMI between current and previous cluster assignments\n",
    "                nmi = normalized_mutual_info_score(prev_cluster_assignments, cluster_assignments)\n",
    "                print(f\"Epoch {epoch + 1} NMI: {nmi:.4f}\")\n",
    "        \n",
    "        # Save current cluster assignments for the next epoch\n",
    "        prev_cluster_assignments = cluster_assignments\n",
    "        prof.step()\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca120d21-27dd-4cc4-b90d-6cccd5f476ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463be082-1104-463c-904a-b366831953a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fe539-a19f-48fe-9c50-82326755e457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eca4eb-8df5-4950-bc19-8bb3f8d839ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepcluster)",
   "language": "python",
   "name": "deepcluster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
